# -*- coding: utf-8 -*-
"""FinalProject 1 .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FMJSexzRN-T4BvPx7K4P_NVMwtyzK8IN
"""

!pip install -q langchain-community sentence-transformers google-generativeai langchain-google-genai faiss-cpu pypdf pdfminer.six

import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
import torch
from google.colab import userdata

GOOGLE_API_KEY=userdata.get("GOOGLE_API_KEY")
os.environ["GOOGLE_API_KEY"]=GOOGLE_API_KEY

embeddings=HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
                                    model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},encode_kwargs={'normalize_embeddings': True},
                                )

llm = ChatGoogleGenerativeAI(model="gemini-pro",temperature=0.3,convert_system_message_to_human=True)

# Create a special PDF loader customized for Bangla documents
class BanglaPDFLoader:


    def __init__(self, file_path):
        self.file_path = file_path  # Stores the PDF file location


    def load(self):
        try:
            # First try using PyPDFLoader
            loader = PyPDFLoader(self.file_path)
            pages = loader.load_and_split()  # Extract pages and split into sections

            # Print the extracted text from first page
            print("=== Extracted Text (PyPDF) ===")
            print(pages[0].page_content[:500] + "...")  # Show first 500 chars

            # Check if text contains Bangla characters
            if self._has_bangla_content(pages[0].page_content):
                return pages  # Return if Bangla found

        except Exception as e:
            # If PyPDF fails, try pdfminer
            print(f"PyPDF failed, trying pdfminer: {e}")

            from pdfminer.high_level import extract_text
            from langchain_core.documents import Document

            # Extract raw text using pdfminer
            text = extract_text(self.file_path)


            print("\n=== Extracted Text (pdfminer) ===")
            print(text[:500] + "...")


            return [Document(page_content=text)] if self._has_bangla_content(text) else []
        return []

    def _has_bangla_content(self, text):
        bangla_chars = set("অআইঈউঊঋএঐওঔকখগঘঙচছজঝঞটঠডঢণতথদধনপফবভমযরলশষসহড়ঢ়য়ৈােীুূৃৄেৈোৌ্ৎংঃ")
        return any(char in bangla_chars for char in text[:500])

def clean_bangla_text(text):
  fixes = {
        "à¦": "া", "à§": "ে", "্র্": "্র", "্‌": "্",
        "": " ", "�": "", "\u200c": "", "\u200d": "",
        "  ": " ", "\t": " ", "\r\n": "\n",
        "ê": "ে", "ô": "ো", "û": "ূ", "î": "ী",
        "à": "া", "é": "ে", "ç": "চ"
    }
  for wrong,correct in fixes.items():
    text=text.replace(wrong,correct)
  return text.strip()

def get_bangla_splitter():
    return RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", "। ", "।", " ", ""],
        keep_separator=True,
        strip_whitespace=False
    )

# 7. Bilingual Prompt Template
BILINGUAL_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """
    You are a Bangla-English bilingual expert. Strictly follow:
    1. Respond in the QUESTION'S LANGUAGE
    2. For Bangla: Use চলিত ভাষা with numbers like ১০০
    3. Keep technical terms (e.g., "AI", "GDP") in original language
    4. If unsure: "প্রদত্ত ডকুমেন্টে তথ্য নেই" / "Info not in document"
    """),
    ("human", """
    DOCUMENT CONTEXT:
    {context}

    QUESTION:
    {question}

    ANSWER (concise and in the question's language):
    """)
])

class BanglaRAGSystem:

  def __init__(self):
    self.embeddings=embeddings
    self.llm=ChatGoogleGenerativeAI(model="gemini-2.5-pro", temperature=0.3)
    self.vector_store=None

  def process_pdf(self,file_path):
    pages=BanglaPDFLoader(file_path).load()
    cleaned_pages = [clean_bangla_text(page.page_content) for page in pages if page.page_content.strip()]

    chunks=get_bangla_splitter().split_documents(pages)
    self.vector_store=FAISS.from_documents(chunks,self.embeddings)


    self.retriever = self.vector_store.as_retriever(
            search_type="mmr",  # Finds diverse relevant chunks
            search_kwargs={"k": 5, "lambda_mult": 0.6}  # Gets 5 best matches
        )

            # Create chain
    self.chain =(
            {"context": self.retriever, "question": RunnablePassthrough()}
            | BILINGUAL_PROMPT
            | self.llm
        )

  def query(self, question):
     return self.chain.invoke(question)

if __name__ == "__main__":
    # Initialize
    rag = BanglaRAGSystem()

    # Upload PDF in Colab
    from google.colab import files
    print("Upload your Bangla PDF file:")
    uploaded = files.upload()
    pdf_file = next(iter(uploaded))

    # Process PDF
    rag.process_pdf(pdf_file)

    # Test questions
    questions = [
        "অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?",
        "কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?",
        "বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?"
    ]

    for q in questions:
        print(f"\nপ্রশ্ন: {q}")
        print(f"উত্তর: {rag.query(q).content}")

import re

def clean_bangla_text(text):
    # Common encoding fixes (PDF/OCR/copy-paste errors)
    encoding_fixes = {
        # Fix vowel encoding issues
        "à¦": "া", "à§": "ে", "à¥": "ী", "à¤": "ি",
        "à¦¾": "া", "à¦¿": "ি", "à¦¸": "স",
        # Fix consonant clusters
        "্র্": "্র", "্রে": "্রে", "্রো": "্রো",
        # Fix invisible characters
        "\u200b": "", "\u200c": "", "\u200d": "", "\ufeff": "",
        # Fix broken punctuation
        "।।": "।", "..": "।", "?.": "?",
        # Fix common OCR mistakes
        "ê": "ে", "ô": "ো", "û": "ূ", "î": "ী",
        "à": "া", "é": "ে", "ç": "চ", "ñ": "ঞ"
    }

    # Apply encoding fixes
    for wrong, correct in encoding_fixes.items():
        text = text.replace(wrong, correct)

    # Step 2: Normalize Unicode compositions (e.g., ক + ্ + ষ = ক্ষ)
    text = text.replace("\u09CD\u09B7", "\u09CD\u09B7")  # ক্ষ fix

    # Step 3: Remove duplicate/irregular whitespace
    text = re.sub(r"\s+", " ", text).strip()

    # Step 4: Fix common conjuncts (যুক্তাক্ষর)
    conjunct_fixes = {
        "ক্‌ষ": "ক্ষ", "স্‌থ": "স্থ", "জ্‌ঞ": "জ্ঞ",
        "ব্‌দ": "ব্দ", "শ্‌র": "শ্র"
    }
    for wrong, correct in conjunct_fixes.items():
        text = text.replace(wrong, correct)

    return text

print("\nইন্টারেক্টিভ মোড শুরু হয়েছে। প্রস্থান করতে 'exit' লিখুন")
while True:
    user_q = input("\nআপনার প্রশ্ন: ")
    cleaned_q = clean_bangla_text(user_q)

    if cleaned_q.lower() in ['exit', 'quit', 'প্রস্থান']:
        break

    response = rag.query(cleaned_q)
    print(f"\nউত্তর:\n{response.content}\n")

import google.generativeai as genai
from google.colab import userdata


GOOGLE_API_KEY = userdata.get("GOOGLE_API_KEY")
genai.configure(api_key=GOOGLE_API_KEY)

for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

!pip install ipynbname